{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnth/mambaforge-pypy3/envs/bentoml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]\n",
      "/home/dnth/mambaforge-pypy3/envs/bentoml/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:513: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_id = \"./models-phi-35-vision\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    _attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_id, trust_remote_code=True, num_crops=16\n",
    ")\n",
    "\n",
    "model = torch.compile(model, mode=\"max-autotune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_image(image_url, model, processor, max_new_tokens=20, temperature=0.0):\n",
    "    image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "    \n",
    "    placeholder = \"<|image_1|>\\n\"\n",
    "    prompt_text = \"Describe the image in concise, focusing on the main subjects, their actions, and the overall setting. Include information about colors, textures, and any notable objects or elements in the background. eliminate filler words, adverbs, and any unnecessary phrases, focusing solely on the core meaning and essential information.\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": placeholder + prompt_text},\n",
    "    ]\n",
    "    prompt = processor.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    with torch.inference_mode(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "      inputs = processor(prompt, image, return_tensors=\"pt\").to(device, dtype=torch.bfloat16)\n",
    "\n",
    "      generation_args = {\n",
    "          \"max_new_tokens\": max_new_tokens,\n",
    "          \"temperature\": temperature,\n",
    "          \"do_sample\": False,\n",
    "      }\n",
    "      \n",
    "      generate_ids = model.generate(**inputs,\n",
    "          eos_token_id=processor.tokenizer.eos_token_id,\n",
    "          **generation_args\n",
    "      )\n",
    "      \n",
    "      # Decode and return the response\n",
    "      generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "      response = processor.batch_decode(generate_ids,\n",
    "          skip_special_tokens=True,\n",
    "          clean_up_tokenization_spaces=False\n",
    "      )[0]\n",
    "      \n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnth/mambaforge-pypy3/envs/bentoml/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image features a blue background with a geometric pattern of overlapping hexagons. In the\n",
      "CPU times: user 2.77 s, sys: 233 ms, total: 3 s\n",
      "Wall time: 1.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "url = \"https://image.slidesharecdn.com/azureintroduction-191206101932/75/Introduction-to-Microsoft-Azure-Cloud-1-2048.jpg\"\n",
    "description = describe_image(url, model, processor, max_new_tokens=20)\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bentoml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
